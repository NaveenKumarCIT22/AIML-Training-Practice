Unit -III Map Reduce And Spark FrameworkMap Reduce: Anatomy of a Map Reduce job Run, Failure, Job Scheduling, Shuffle and sort, Task Exception, Map Reduce Types and formats, Map Reduce feactures, Map Reduce programming: I/O formats, Map side join, Reduce side join,Secondary sorting, Pipelining Map reduce jobs.Spark frameworks: Introduction to GPU Computing,CUDA programming model, CUDA API, Simple matrix,Multiplication in CUDA, CUDA memory model, Shared memory matrix multiplication,Additional CUDA API Features.Anatomy of a Map Reduce job RunThe term "big data analytics" refers to the application of cutting-edge analytical methods to very large, diversified big data sets, which may comprise structured, semi-structured, and unstructured data from many sources and range in size from terabytes to zettabytes. The high volume, high velocity, and high variety of big data are its characteristics. Due to the influence of artificial intelligence (AI), mobile technology, social media, and the Internet of Things (IoT), data sources are becoming more complicated than those for traditional data.echnology and Tools of Analytics: Anatomy of Map Reduce job Run
There are two key parts of the Hadoop Framework:MapReduce for data processing Hadoop Distributed File System (HDFS) for data storage.In this article, we'll talk about Apache Hadoop's MapReduce Job Anatomy. On a Hadoop cluster, a set of Map and Reduce processes work together to complete a typical Hadoop MapReduce job. 
The following is the execution flow:Small data subsets are created from the input data.On these data partitions, map tasks operate.After a first step known as "shuffle," the intermediate input data from Map jobs is then provided to Reduce tasks.To produce the output of a MapReduce Job, the Reduce task(s) works on this intermediate data.To generate the necessary data, the 'teragen' programm starts two map tasks and three reduction activities.The records are produced by the Map tasks.A combiner task receives the created records as input. The 'combiner' is a process in between.We'll go into more detail on combiner in a subsequent blog. Consider it currently as a transitional procedure before reducing the task.Records from the combiner's output are used as input for a reduce job.The reducer task compiles the data and creates the output records in the end.How a MapReduce Job Run WorksThere is only one function you need to use to start a MapReduce job: submit() on a Job object (you can also use wait ForCompletion(), which submits the task if it hasn't already and then waits for it to finish).This method call hides a lot of processing that occurs in the background. This section explains the procedures Hadoop goes through to complete a task.In the provided figure, the entire procedure is depicted. There are five distinct entities at the highest level:MapReduce job submission by the client.The cluster's compute resource allocation is managed by the YARN resource management.Launching and keeping an eye on the computing containers on cluster machines are the YARN node managers.The master MapReduce application, which manages the tasks involved in running the MapReduce operation. Running in containers that are scheduled by the resource manager and controlled by the node managers are the application master and the MapReduce tasks.The distributed filesystem is used to distribute job files among the many entities.
