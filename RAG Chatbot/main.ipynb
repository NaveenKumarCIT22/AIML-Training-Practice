{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain-huggingface in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (0.1.2)\n",
      "Requirement already satisfied: chroma in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (4.46.2)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (0.3.7)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "     ------------------------------------ 298.0/298.0 kB 836.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (3.11.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (0.1.143)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain-huggingface) (0.26.2)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain-huggingface) (0.20.3)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain-huggingface) (3.3.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain-core) (4.10.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.9.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.5.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (9.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.0.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.8.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (0.4.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.0.1)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-huggingface chroma transformers langchain-core langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.10.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.8.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\navee\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "creds = {}\n",
    "with open(\"creds.json\") as f:\n",
    "    creds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit -III Map Reduce And Spark FrameworkMap Reduce: Anatomy of a Map Reduce job Run, Failure, Job Scheduling, Shuffle and sort, Task Exception, Map Reduce Types and formats, Map Reduce feactures, Map Reduce programming: I/O formats, Map side join, Reduce side join,Secondary sorting, Pipelining Map reduce jobs.Spark frameworks: Introduction to GPU Computing,CUDA programming model, CUDA API, Simple matrix,Multiplication in CUDA, CUDA memory model, Shared memory matrix multiplication,Additional CUDA API Features.Anatomy of a Map Reduce job RunThe term \"big data analytics\" refers to the application of cutting-edge analytical methods to very large, diversified big data sets, which may comprise structured, semi-structured, and unstructured data from many sources and range in size from terabytes to zettabytes. The high volume, high velocity, and high variety of big data are its characteristics. Due to the influence of artificial intelligence (AI), mobile technology, social media, and the Internet of Things (IoT), data sources are becoming more complicated than those for traditional data.echnology and Tools of Analytics: Anatomy of Map Reduce job Run\n",
      "There are two key parts of the Hadoop Framework:MapReduce for data processing Hadoop Distributed File System (HDFS) for data storage.In this article, we'll talk about Apache Hadoop's MapReduce Job Anatomy. On a Hadoop cluster, a set of Map and Reduce processes work together to complete a typical Hadoop MapReduce job. \n",
      "The following is the execution flow:Small data subsets are created from the input data.On these data partitions, map tasks operate.After a first step known as \"shuffle,\" the intermediate input data from Map jobs is then provided to Reduce tasks.To produce the output of a MapReduce Job, the Reduce task(s) works on this intermediate data.To generate the necessary data, the 'teragen' programm starts two map tasks and three reduction activities.The records are produced by the Map tasks.A combiner task receives the created records as input. The 'combiner' is a process in between.We'll go into more detail on combiner in a subsequent blog. Consider it currently as a transitional procedure before reducing the task.Records from the combiner's output are used as input for a reduce job.The reducer task compiles the data and creates the output records in the end.How a MapReduce Job Run WorksThere is only one function you need to use to start a MapReduce job: submit() on a Job object (you can also use wait ForCompletion(), which submits the task if it hasn't already and then waits for it to finish).This method call hides a lot of processing that occurs in the background. This section explains the procedures Hadoop goes through to complete a task.In the provided figure, the entire procedure is depicted. There are five distinct entities at the highest level:MapReduce job submission by the client.The cluster's compute resource allocation is managed by the YARN resource management.Launching and keeping an eye on the computing containers on cluster machines are the YARN node managers.The master MapReduce application, which manages the tasks involved in running the MapReduce operation. Running in containers that are scheduled by the resource manager and controlled by the node managers are the application master and the MapReduce tasks.The distributed filesystem is used to distribute job files among the many entities.\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(creds[\"filepath\"])\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a076a37e3b3645ef988971bcf2f17c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77df20a52bf540909f6aa1f1082385c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cd3f86878a4a91af7a5ac2949eef8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6158fdc06c4d482f88d9f9aa62f0cbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d137e7b2754822a9c2f17af78c5fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e098e8ac729a46d698bcdd2af334e054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navee\\AppData\\Local\\Temp\\ipykernel_10352\\738916483.py:3: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "# create vector store if not exists\n",
    "db = Chroma.from_documents(docs, embeddings, persist_directory=\"db\")\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navee\\AppData\\Local\\Temp\\ipykernel_10352\\3761223157.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents(\"what is map?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': 'D:/NK_Studies/Sem5/BDA/bda3.pdf'}, page_content='Unit -III Map Reduce And Spark FrameworkMap Reduce: Anatomy of a Map Reduce job Run, Failure, Job Scheduling, Shuffle and sort, Task Exception, Map Reduce Types and formats, Map Reduce feactures, Map Reduce programming: I/O formats, Map side join, Reduce side join,Secondary sorting, Pipelining Map reduce jobs.Spark frameworks: Introduction to GPU Computing,CUDA programming model, CUDA API, Simple matrix,Multiplication in CUDA, CUDA memory model, Shared memory matrix multiplication,Additional CUDA API Features.Anatomy of a Map Reduce job RunThe term \"big data analytics\" refers to the application of cutting-edge analytical methods to very large, diversified big data sets, which may comprise structured, semi-structured, and unstructured data from many sources and range in size from terabytes to zettabytes. The high volume, high velocity, and high variety of big data are its characteristics. Due to the influence of artificial intelligence (AI), mobile technology, social media, and the Internet of Things (IoT), data sources are becoming more complicated than those for traditional data.echnology and Tools of Analytics: Anatomy of Map Reduce job Run\\nThere are two key parts of the Hadoop Framework:MapReduce for data processing Hadoop Distributed File System (HDFS) for data storage.In this article, we\\'ll talk about Apache Hadoop\\'s MapReduce Job Anatomy. On a Hadoop cluster, a set of Map and Reduce processes work together to complete a typical Hadoop MapReduce job. \\nThe following is the execution flow:Small data subsets are created from the input data.On these data partitions, map tasks operate.After a first step known as \"shuffle,\" the intermediate input data from Map jobs is then provided to Reduce tasks.To produce the output of a MapReduce Job, the Reduce task(s) works on this intermediate data.To generate the necessary data, the \\'teragen\\' programm starts two map tasks and three reduction activities.The records are produced by the Map tasks.A combiner task receives the created records as input. The \\'combiner\\' is a process in between.We\\'ll go into more detail on combiner in a subsequent blog. Consider it currently as a transitional procedure before reducing the task.Records from the combiner\\'s output are used as input for a reduce job.The reducer task compiles the data and creates the output records in the end.How a MapReduce Job Run WorksThere is only one function you need to use to start a MapReduce job: submit() on a Job object (you can also use wait ForCompletion(), which submits the task if it hasn\\'t already and then waits for it to finish).This method call hides a lot of processing that occurs in the background. This section explains the procedures Hadoop goes through to complete a task.In the provided figure, the entire procedure is depicted. There are five distinct entities at the highest level:MapReduce job submission by the client.The cluster\\'s compute resource allocation is managed by the YARN resource management.Launching and keeping an eye on the computing containers on cluster machines are the YARN node managers.The master MapReduce application, which manages the tasks involved in running the MapReduce operation. Running in containers that are scheduled by the resource manager and controlled by the node managers are the application master and the MapReduce tasks.The distributed filesystem is used to distribute job files among the many entities.'),\n",
       " Document(metadata={'page': 6, 'source': 'D:/NK_Studies/Sem5/BDA/bda3.pdf'}, page_content='and the value is appended to a list of values. So, the Shuffle output format will be a map <key, List<list of values>>.  The Mapper output key will be combined and sorted. Reduce phase: The result of the shuffle and sorting phases is sent as input into the Reducer phase, which processes the list of values. Each key could be routed to a distinct Reducer. Reducers can set the value, which is then consolidated in the final output of a MapReduce job and saved in HDFS as the final output.There are two format Map Reduce are : Input format Map Reduces Output format Map Reduce 1. Input Format in Map Reduce: Hadoop InputFormat provides the input specification for Map-Reduce job execution. InputFormat specifies how to divide and read input files. InputFormat is the initial phase in MapReduce job execution. It is also in charge of generating input splits and separating them into records. One of the core classes in MapReduce is InputFormat, which provides the following functionality: InputFormat determines which files or other objects to accept as input. It also specifies data splits. It specifies the size of each Map task as well as the potential execution server. The RecordReader is defined by Hadoop InputFormat. It is also in charge of reading actual records from input files. ExampleAn example of MapReduce types of InputFormat is given below:import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class Text_input_output_example {    public static class MyMapper extends Mapper<LongWritable, Text, Text, Text> {        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {      // Put the map logic here.      context.write(new Text(\"OutputKey\"), value);    }  }\\n    public static void main(String[] args) throws Exception {    // Construct a Hadoop configuration.    Configuration conf = new Configuration();        Job job = Job.getInstance(conf, \"Text_input_output_example\");    job.setJarByClass(Text_input_output_example.class);        // TextInputFormat should be used as the input format class.    job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(MyMapper.class);      job.setMapOutputKeyClass(Text.class);    job.setMapOutputValueClass(Text.class);        // TextOutputFormat should be used as the output format class.    job.setOutputFormatClass(TextOutputFormat.class);        job.setOutputKeyClass(Text.class);    job.setOutputValueClass(Text.class);        TextInputFormat.addInputPath(job, new Path(\"input_dir\"));    TextOutputFormat.setOutputPath(job, new Path(\"output_dir\"));        // Wait for the job to finish before exiting.    System.exit(job.waitForCompletion(true) ? 0 : 1);  }} The code above uses the TextInputFormat to read input data from plain text files. You can specify your map logic in the MyMapper class, which is the mapper implementation. The result is saved as plain text files because the output format is set to TextOutputFormat.Types of InputFormat in MapReduceIn Hadoop, there are various MapReduce types for InputFormat that are used for various purposes.  Let us now look at the MapReduce types of InputFormat:FileInputFormatIt serves as the foundation for all file-based InputFormats. FileInputFormat also provides the input directory, which contains the location of the data files. When we start a MapReduce task, FileInputFormat returns a path with files to read. This InpuFormat will read all files. Then it divides these files into one or more InputSplits.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 2},\n",
    "                            search_type=\"similarity\",\n",
    "                            )\n",
    "retriever.get_relevant_documents(\"what is map?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        max_length=500,\n",
    "        temperature=0.8,\n",
    "        huggingfacehub_api_token=creds[\"hf_token\"],\n",
    "    )\n",
    "# from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# llm = LlamaCpp(\n",
    "#     model_path=creds[\"modelpath\"],\n",
    "#     callback_manager=callback_manager,\n",
    "#     verbose=True,  # Verbose is required to pass to the callback manager\n",
    "#     max_context_length=4096,\n",
    "#     max_new_tokens=1024,\n",
    "#     temperature=0.75,\n",
    "#     top_p=1,\n",
    "#     n_gpu_layers=1,\n",
    "#     n_batch=512,\n",
    "#     n_ctx=4096\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a big data analytics expert. These Human will ask you a questions and doubts on the topic. \n",
    "Use following piece of context to answer the question. \n",
    "If you don't know the answer, just say you don't know. \n",
    "Keep the answer within 5 sentences and concise.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=template, \n",
    "  input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "  {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "  | prompt \n",
    "  | llm\n",
    "  | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map is a process in Apache Hadoop's MapReduce framework that operates on small data subsets, called input splits. It applies a user-defined function, called a mapper, to each input record to generate output records. The output records are then shuffled and sorted based on their keys before being processed by the reduce phase.\n"
     ]
    }
   ],
   "source": [
    "qn = input(\"What is it you want? >> \")\n",
    "res = rag_chain.invoke(qn)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
