{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "# from langchain.llms import LlamaCpp\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "creds = {}\n",
    "with open(\"creds.json\") as f:\n",
    "    creds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit -III Map Reduce And Spark Framework\n",
      "Map Reduce: Anatomy of a Map Reduce job Run, Failure, Job Scheduling, Shuffle and sort, \n",
      "Task Exception, Map Reduce Types and formats, Map Reduce feactures, Map Reduce \n",
      "programming:  I/O formats, Map side join, Reduce side join,Secondary sorting, Pipelining Map \n",
      "reduce jobs.\n",
      "Spark frameworks:  Introduction to GPU Computing,CUDA programming model, CUDA API, \n",
      "Simple matrix,Multiplication in CUDA, CUDA memory model, Shared memory matrix \n",
      "multiplication,Additional CUDA API Features.\n",
      "Anatomy of a Map Reduce job Run\n",
      "The term \"big data analytics\" refers to the application of cutting-edge analytical methods to very \n",
      "large, diversified big data sets, which may comprise structured, semi-structured, and unstructured \n",
      "data from many sources and range in size from terabytes to zettabytes. The high volume, high \n",
      "velocity, and high variety of big data are its characteristics. Due to the influence of artificial \n",
      "intelligence (AI), mobile technology, social media, and the Internet of Things (IoT), data sources \n",
      "are becoming more complicated than those for traditional data.\n",
      "echnology and Tools of Analytics: Anatomy of Map Reduce job Run\n",
      "There are two key parts of the Hadoop Framework:\n",
      "MapReduce for data processing \n",
      "Hadoop Distributed File System (HDFS) for data storage.\n",
      "In this article, we'll talk about Apache Hadoop's MapReduce Job Anatomy. On a Hadoop cluster, \n",
      "a set of Map and Reduce processes work together to complete a typical Hadoop MapReduce job. \n",
      "The following is the execution flow:\n",
      "Small data subsets are created from the input data.\n",
      "On these data partitions, map tasks operate.\n",
      "After a first step known as \"shuffle,\" the intermediate input data from Map jobs is then provided \n",
      "to Reduce tasks.\n",
      "To produce the output of a MapReduce Job, the Reduce task(s) works on this intermediate data.\n",
      "To generate the necessary data, the 'teragen' programm starts two map tasks and three reduction \n",
      "activities.\n",
      "The records are produced by the Map tasks.\n",
      "A combiner task receives the created records as input. The 'combiner' is a process in \n",
      "between.\n",
      "We'll go into more detail on combiner in a subsequent blog. Consider it currently as a \n",
      "transitional procedure before reducing the task.\n",
      "Records from the combiner's output are used as input for a reduce job.\n",
      "The reducer task compiles the data and creates the output records in the end.\n",
      "How a MapReduce Job Run Works\n",
      "There is only one function you need to use to start a MapReduce job: submit() on a Job object \n",
      "(you can also use wait ForCompletion(), which submits the task if it hasn't already and then waits \n",
      "for it to finish).This method call hides a lot of processing that occurs in the background. This \n",
      "section explains the procedures Hadoop goes through to complete a task.\n",
      "In the provided figure, the entire procedure is depicted. There are five distinct entities at the \n",
      "highest level:\n",
      "MapReduce job submission by the client.\n",
      "The cluster's compute resource allocation is managed by the YARN resource \n",
      "management.\n",
      "Launching and keeping an eye on the computing containers on cluster machines are the \n",
      "YARN node managers.\n",
      "The master MapReduce application, which manages the tasks involved in running the \n",
      "MapReduce operation. Running in containers that are scheduled by the resource manager \n",
      "and controlled by the node managers are the application master and the MapReduce \n",
      "tasks.\n",
      "The distributed filesystem is used to distribute job files among the many entities.\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(creds[\"filepath\"])\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector store if not exists\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(\n",
    "    model=creds[\"modelpath\"],\n",
    "    model_type=\"llama\",\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a big data analytics expert. These Human will ask you a questions and doubts on the topic. \n",
    "Use following piece of context to answer the question. \n",
    "If you don't know the answer, just say you don't know. \n",
    "Keep the answer within 5 sentences and concise.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=template, \n",
    "  input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "  {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "  | prompt \n",
    "  | llm\n",
    "  | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (3336) exceeded maximum context length (512).\n"
     ]
    }
   ],
   "source": [
    "qn = input(\"What is it you want? >> \")\n",
    "res = rag_chain.invoke(qn)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
